{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "layB9pEvUwqf"
   },
   "source": [
    "# Lab: Batch Normalization and Transfer Learning\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to _layers within_ the network. \n",
    "\n",
    "> It's called **batch** normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current *batch*.\n",
    "\n",
    "We will first analyze the effect of Batch Normalization (BN) in a simple NN with dense layers. Then you will be able to incorportate BN into the CNN that you designed in the first part of CNNs Lab. \n",
    "\n",
    "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyfhjMsUwqh"
   },
   "source": [
    "## Batch Normalization in PyTorch<a id=\"implementation_1\"></a>\n",
    "\n",
    "This section of the notebook shows you one way to add batch normalization to a neural network built in PyTorch. \n",
    "\n",
    "The following cells import the packages we need in the notebook and load the MNIST dataset to use in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FN3FRcAUwqi"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORTgEv3CUwqr"
   },
   "outputs": [],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training  data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlate1DnUwqx"
   },
   "source": [
    "### Neural network classes\n",
    "\n",
    "The following class, `MLP`, allows us to create identical neural networks **with and without batch normalization** to compare. We are defining a simple NN with **two dense layers** for classification; this design choice was made to support the discussion related to batch normalization and not to get the best classification accuracy.\n",
    "\n",
    "Two importants points about BN:\n",
    "\n",
    "- We use PyTorch's [BatchNorm1d](https://pytorch.org/docs/stable/nn.html#batchnorm1d). This is the function you use to operate on linear layer outputs; you'll use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) for 2D outputs like filtered images from convolutional layers. \n",
    "- We add the batch normalization layer **before** calling the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OW8ZWM1Uwqz"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dimx,hidden1,hidden2,nlabels,use_batch_norm): #Nlabels will be 10 in our case\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        self.output1 = nn.Linear(dimx,hidden1)\n",
    "        \n",
    "        self.output2 = nn.Linear(hidden1,hidden2)        \n",
    "        \n",
    "        self.output3 = nn.Linear(hidden2,nlabels)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        if self.use_batch_norm:\n",
    "\n",
    "            self.batch_norm1 = nn.BatchNorm1d(hidden1)\n",
    "            \n",
    "            self.batch_norm2 = nn.BatchNorm1d(hidden2)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.output1(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output2(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm2(x)        \n",
    "        x = self.relu(x)\n",
    "        x = self.output3(x)\n",
    "        x = self.logsoftmax(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DRvyjGMUwq3"
   },
   "source": [
    "> **Exercise:** \n",
    "> \n",
    "> - Create a validation set with the 20% of training set\n",
    "> - Extend the class above to incorporate a training method where both training and validation losses are computed, and a method to evaluate the classification performance on a given set\n",
    "\n",
    "**Note:** As we do with Dropout, for BN we have to call the methods `self.eval()` and `self.train()` in both validation and training. Setting a model to evaluation mode is important for models with batch normalization layers!\n",
    "\n",
    ">* Training mode means that the batch normalization layers will use **batch** statistics to calculate the batch norm. \n",
    "* Evaluation mode, on the other hand, uses the estimated **population** mean and variance from the entire training set, which should give us increased performance on this test data!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UAT1IgvUwq4"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nloclzQLUwrB"
   },
   "source": [
    "### Create two different models for testing\n",
    "\n",
    "* `net_batchnorm` uses batch normalization applied to the output of its hidden layers\n",
    "* `net_no_norm` does not use batch normalization\n",
    "\n",
    "Besides the normalization layers, everthing about these models is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFQ2pcH8UwrG"
   },
   "source": [
    "> **Exercise:** Train both models and compare the evolution of the train/validation loss in both cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfjYlvmHasYs"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfyqTIjVUwrR"
   },
   "source": [
    "---\n",
    "### Considerations for other network types\n",
    "\n",
    "This notebook demonstrates batch normalization in a standard neural network with fully connected layers. You can also use batch normalization in other types of networks, but there are some special considerations.\n",
    "\n",
    "#### ConvNets\n",
    "\n",
    "Convolution layers consist of multiple feature maps. (Remember, the depth of a convolutional layer refers to its number of feature maps.) And the weights for each feature map are shared across all the inputs that feed into the layer. Because of these differences, batch normalizing convolutional layers requires batch/population mean and variance per feature map rather than per node in the layer.\n",
    "\n",
    "> To apply batch normalization on the outputs of convolutional layers, we use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d). To use it, we simply state the **number of input feature maps**. I.e. `nn.BatchNorm2d(num_features=nmaps)`\n",
    "\n",
    "\n",
    "#### RNNs\n",
    "\n",
    "Batch normalization can work with recurrent neural networks, too, as shown in the 2016 paper [Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025). It's a bit more work to implement, but basically involves calculating the means and variances per time step instead of per layer. You can find an example where someone implemented recurrent batch normalization in PyTorch, in [this GitHub repo](https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXRabJ90UwrT"
   },
   "source": [
    "> **Exercise:** Using CIFAR10 database, incorporate BN to your solution of the previous lab on CNNs. Compare the results with and without BN!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLHhFPGpXQLk"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning in Pytorch\n",
    "\n",
    "In this section of the lab, you'll learn how to use pre-trained CNN networks to solved challenging problems in computer vision. Specifically, you'll use networks trained on [ImageNet](http://www.image-net.org/) [available from torchvision](http://pytorch.org/docs/0.3.0/torchvision/models.html). ImageNet is a massive dataset with over 1 million labeled images in 1000 categories. \n",
    "\n",
    "Once trained over Imagenet, CNNs models work astonishingly well as feature detectors for images they weren't trained on. Using a pre-trained network on images not in the training set is called transfer learning. Here we'll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy. With [`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) you can download these pre-trained networks and use them in your applications. We'll include `models` in our imports now.\n",
    "\n",
    "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://www.pyimagesearch.com/wp-content/uploads/2016/08/knn_kaggle_dogs_vs_cats_sample.jpg\", width=400, height=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.\n",
    "\n",
    "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the notebook in Google Colab, we mount our Drive folder to later access the Cat & Dogs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = '/content/drive/My Drive/Neural_Networks_20_21_Grado_Datos/Practicas/lab3/'  # UPDATE THIS ACCORDING TO WHERE YOU HAVE SAVED THE DATABASE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data set. Incorporate Data Augmentation\n",
    "\n",
    "#### Dataset folder structure\n",
    "\n",
    "The easiest way to load you own training/test image dataset is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n",
    "\n",
    "```python\n",
    "dataset = datasets.ImageFolder('path/to/data', transform=transform)\n",
    "```\n",
    "\n",
    "where `'path/to/data'` is the file path to the data directory and `transform` is a sequence of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. **More about this below**. ImageFolder expects the files and directories to be constructed like so:\n",
    "```\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/asd932_.png\n",
    "```\n",
    "\n",
    "where each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. \n",
    "\n",
    "You can check that the provided dataset has the required structure.\n",
    "\n",
    "#### Data augmentation\n",
    "\n",
    "A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above). \n",
    "\n",
    "A common technique to enforce the invariancy of the CNN and obtain a more robust classifier is [Data Augmentation](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced), which seeks to train the neural network with additional synthetically modified data, generated by introducing randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc. We do this by defining a torchvision `transform`, and you can learn about all the transforms that are used to pre-process and augment data, [here](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "This type of data augmentation should add some positional variety to these images, so that when we train a model on this data, it will be robust in the face of geometric changes (i.e. it will recognize a ship, no matter which direction it is facing). It's recommended that you choose one or two transforms.\n",
    "\n",
    "\n",
    "To randomly rotate, scale and crop, then flip your images you would define your transforms like this:\n",
    "\n",
    "```python\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "```\n",
    "\n",
    "You'll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n",
    "\n",
    "```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n",
    "\n",
    "Subtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.\n",
    "\n",
    "You can find a list of all [the available transforms here](http://pytorch.org/docs/0.3.0/torchvision/transforms.html). When you're testing however, you'll want to use images that aren't altered other than normalizing. So, for validation/test images, you'll typically just resize and crop.\n",
    "\n",
    "\n",
    "In this notebook, we will use [DenseNet](http://pytorch.org/docs/0.3.0/torchvision/models.html#id5) as feature extractor. It requires the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`.\n",
    "\n",
    "With the following code, we load the dataset of cats & dogs, perform the data augmentation trasformations, and normalize the input images according to DenseNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = path_to_folder + 'Cat_Dog_data_reduced'\n",
    "\n",
    "# Train Set\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Test Set\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize some images (they look weird because of the normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(testloader)\n",
    "\n",
    "images, labels = next(data_iter)\n",
    "fig, axes = plt.subplots(figsize=(10,4), ncols=6)\n",
    "for ii in range(6):\n",
    "    ax = axes[ii]\n",
    "    ax.imshow(images[ii,:,:,:].numpy().transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DenseNet and modifing the last classification layer\n",
    "\n",
    "\n",
    "[DensetNet](https://arxiv.org/abs/1608.06993) was proposed in the 2017 IEEE Conference on Computer Vision and Pattern Recognition, and it is one of the most popular CNNs for computer vision applications. \n",
    "\n",
    "The models is huge and contains millions of parameters. [This excellent post](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a) explains in detail the DenseNet structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1138/1*GeK21UAbk4lEnNHhW_dgQA.png\", width=400, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/2846/1*SsphOqMwglCVGDWB-jdT5Q.png\", width=800, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in a model such as [DenseNet](http://pytorch.org/docs/0.3.0/torchvision/models.html#id5). Let's print out the model architecture so we can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet121(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is built out of two main parts, the features and the classifier. The features part is a stack of convolutional layers and overall works as a feature detector that can be fed into a classifier. The classifier part is a single fully-connected layer `(classifier): Linear(in_features=1024, out_features=1000)`. This layer was trained on the ImageNet dataset, so **it won't work for our specific problem**. That means we need to replace the classifier, but the features will work perfectly on their own. In general, think about pre-trained networks as amazingly good feature detectors that can be used as the input for simple feed-forward classifiers.\n",
    "\n",
    "Replacing the classifier is straightforward. We go step by step. \n",
    "\n",
    "> **Excercise:** Create a class for a NN binary classifier with two dense layers. Use 500 hidden units as the output dimension of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dimx=1024,hidden=500,nlabels=2): \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.output1 = #YOUR CODE HERE\n",
    "    \n",
    "        self.output2 = #YOUR CODE HERE\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)                                                             \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        #YOUR CODE HERE \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate the class `MLP` we just created and replace the classifier part of DenseNet. Also, we freeze the rest of DenseNet parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier = MLP(1024)\n",
    "    \n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model built, we need to train the classifier. However, now we're using a **really deep** neural network. If you try to train this on a CPU like normal, it will take a long, long time. Instead, we're going to use the GPU to do the calculations. The linear algebra computations are done in parallel on the GPU leading to 100x increased training speeds. It's also possible to train on multiple GPUs, further decreasing training time.\n",
    "\n",
    "> **Exercise**: Complete the following code for a class that trains the defined model.\n",
    "\n",
    "Note that, to make things easier, this time we pass the instatiated model as an input to the class!\n",
    "\n",
    "You will see that training is slow (every epoch can take more than one hour). Hence, the following code is prepared to be trained for only a few minibatches (we specify the number of SGD iterations, not epochs). In any case, since the features learnt by Densenet are very discriminative, the classifier performs well within only a few iterations! Also, to save time we do not compute validation loss. But to do it right and evaluate overfitting, you should implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tran_Eval():\n",
    "    \n",
    "    \n",
    "    def __init__(self,model,maxiter=500,lr=0.001):\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.optim = optim.Adam(self.model.classifier.parameters(), self.lr)\n",
    "        \n",
    "        self.max_iter = maxiter\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()             \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "        self.valid_loss_during_training = []\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def trainloop(self,trainloader):\n",
    "        \n",
    "        # Optimization Loop\n",
    "        \n",
    "        it_images = iter(trainloader)\n",
    "        \n",
    "        running_loss = 0.\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for e in range(int(self.max_iter)):\n",
    "\n",
    "          \n",
    "            images,labels = next(it_images)\n",
    "            \n",
    "            \n",
    "            # Move input and label tensors to the default device\n",
    "            images, labels = images.to(self.device), labels.to(self.device)  \n",
    "        \n",
    "            self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
    "\n",
    "            out =  #YOUR CODE HERE\n",
    "\n",
    "            \n",
    "            loss =  #YOUR CODE HERE\n",
    "\n",
    "            running_loss = loss.item()\n",
    "\n",
    "            #Your code here\n",
    "            loss.backward()\n",
    "\n",
    "            #Your code here\n",
    "            self.optim.step()\n",
    "\n",
    "            self.loss_during_training.append(running_loss)\n",
    "            \n",
    "            print('Batch %d of %d finished. Loss %f' %(e,self.max_iter,running_loss))            \n",
    "        \n",
    "                \n",
    "    def eval_performance(self,dataloader,num_batches):\n",
    "\n",
    "        accuracy = 0\n",
    "        \n",
    "        it_images = iter(dataloader)\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            for e in range(int(num_batches)):\n",
    "              \n",
    "                print('Batch %d of %d' %(e,num_batches))\n",
    "              \n",
    "                images,labels = next(it_images)\n",
    "                \n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = #YOUR CODE HERE\n",
    "                probs = #YOUR CODE HERE\n",
    "\n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                equals = (top_class == labels.view(images.shape[0], 1))\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        return accuracy/num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Train the model for 10 iterations. Compute the train/test performance in the first 5 batches of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "STUDENT_Lab_3_Part_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
