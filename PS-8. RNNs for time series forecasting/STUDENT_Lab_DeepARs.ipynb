{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Cs348bG_XKh"
   },
   "source": [
    "# Lab: RNNs for Time Series Prediction\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "In this notebook, you will deploy a simple probabilistic model using a RNN to infer the probability distribution of a time-series. Given a set of signals  $\\mathcal{D}=\\{X^1,X^2,\\ldots,X^N\\}$ where $$X^i = [X_0^i,X_1^i,\\ldots,X_T^i]$$\n",
    "is the $i$-th signal, we will train a probabilistic model of the form\n",
    "$$p(X|X_0) = \\prod_{t=1}^T p(X_t|\\mathbf{X}_{0:t-1})$$\n",
    "where $\\mathbf{X}_{0:t-1}=[X_0,X_1,\\ldots,X_{t-1}]$ and every conditional factor $p(X_t|\\mathbf{X}_{0:t-1})$ corresponds to a Gaussian distribution with mean and variance obtained **from the state of a RNN** with input $X_{t-1}$. The RNN state $\\mathbf{h}_{t-1}$ is a projection of the the signal up to time $t-2$, i.e, $\\mathbf{X}_{0:t-2}$:\n",
    "\n",
    "$$p(X_t|X_{t-1},\\mathbf{X}_{0:t-2}) = \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2+g_{RNN}(X_{t-1},\\mathbf{h}_{t-1})\\right)$$,\n",
    "where $\\sigma^2$ is a constant basal variance that we treat as an hiper-parameter. During training, we approximate the expected loss at time $t$\n",
    "$$\\mathcal{L}_t=\\mathbb{E}_{\\hat{X}_t\\sim p(X_t|X_{t-1},\\mathbf{X}_{0:t-2})}[(X_t-\\hat{X}_t)^2]$$\n",
    "using a single sample of $p(X_t|X_{t-1},\\mathbf{X}_{0:t-2})$. The global loss is accumulated during the whole signal length:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{T}\\sum_{t=1}^T \\mathcal{L}_t$$\n",
    "\n",
    "**Important:** during training, we **feed the RNN with the true values of the signal**. Namely, at each time point $\\mathbf{h}_{t-1}$ is calculated with the true values of the signal. This strategy is known as **teaching forcing**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Px-Jw5Rc_XKm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdvRg05D_XKn"
   },
   "source": [
    "## Part I. Create a synthetic database\n",
    "\n",
    "We will generate $N$ target signals of length $T$ time steps. We generate each signal as one realization of the following autoregressive model\n",
    "\\begin{align}\n",
    "X_{t}=c+\\sum_{i=1}^{p} \\varphi_{i} X_{t-i}+\\varepsilon_{t}\n",
    "\\end{align}\n",
    "where $\\varepsilon_{t}\\sim \\mathcal{N}(0,\\sigma_{\\epsilon})$ and $c$,$\\varphi_{i}$ $i=1,\\ldots,p$ are real coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8t-ToUT1s6yR",
    "outputId": "c6131486-bb4a-473f-cf52-16cf0f16a619"
   },
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtYOM-rgtFmG"
   },
   "source": [
    "We use the library [statsmodel](https://www.statsmodels.org/stable/index.html) to generate the above signals ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yQQmplv_XKn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We  statsmodels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "maparams = []\n",
    "arparams = np.array([.35, .35,0.75,-0.65,-0.15,0.01]) #phi coefficients\n",
    "\n",
    "ar = np.r_[1, -arparams]\n",
    "ma = np.r_[1, maparams]\n",
    "model = sm.tsa.ArmaProcess(ar, ma) #We use c=0 y and unit variance for the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGg7_pritc-3"
   },
   "outputs": [],
   "source": [
    "N = 1000 \n",
    "\n",
    "T = 300\n",
    "\n",
    "X = model.generate_sample((N,T),axis=1,burnin=50) #We ignore the 50 first samples of every signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wK6266lstm0K",
    "outputId": "67448298-84ed-47e9-85ca-b97cad97c3f2"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29u3RFx2ttgG"
   },
   "source": [
    "Given the set of signals, let's create the targets just like the original offset signals:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGdSh9YMtsbk"
   },
   "outputs": [],
   "source": [
    "Y = X[:,1:] \n",
    "X = X[:,:-1] \n",
    "\n",
    "T -=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_40KtVWu_XKo"
   },
   "source": [
    "Lets plot one of the signals versus the *target*, which is the same signal but shifted to the right ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "4KP4LOzF_XKo",
    "outputId": "00cdde26-9128-486e-b603-e96f362617d3"
   },
   "outputs": [],
   "source": [
    "# Plot the signal \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.arange(T), X[1,:T], 'r.--', label='input, x',ms=10) # x\n",
    "plt.plot(np.arange(T), Y[1,:T], 'b.-', label='target, y',ms=10) # y\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Plot the signal (20 first steps)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.arange(20), X[1,:20], 'r.--', label='input, x',ms=10) # x\n",
    "plt.plot(np.arange(20), Y[1,:20], 'b.-', label='target, y',ms=10) # y\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJZ1FOlL_XKp"
   },
   "source": [
    "## Part II. Define RNN\n",
    "\n",
    "Next, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, which takes in a number of parameters:\n",
    "* **input_size** - the size of the input\n",
    "* **hidden_dim** - the dimension of the RNN output and the hidden state\n",
    "* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a **stacked RNN** \n",
    "\n",
    "\n",
    "This is an example of a stacked RNN\n",
    "\n",
    "<img src=\"https://yiyibooks.cn/__src__/wizard/nmt-tut-neubig-2017_20180721165003_deleted/img/6-5.jpg\" width=\"40%\"> \n",
    "\n",
    "\n",
    "If you take a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn), you will see that `nn.RNN` only provides the actual computation of the hidden states along time\n",
    "\\begin{align}\n",
    "h_{t}=g \\left(W_{i h} x_{t}+b_{i h}+W_{h h} h_{(t-1)}+b_{h h}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Then we'll add a last, fully-connected layer to get the output size that we want. For simplicity, **the input to this dense layer is the state $h_t$ of the RNN**.\n",
    "\n",
    "You have to pay special attention to the dimensions of the input/output tensors of the RNN. **Check the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn)**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fphdMpoa_XKq"
   },
   "source": [
    "The following class implements a class where \n",
    "- An input signal of dimension `input_size` is processed by a RNN. As a result, we obtain a sequence of states $\\mathbf{h}_{t}$, from $t=1$ to $t=T$.\n",
    "- We process each state with **two linear layers** to estimate the mean and variance of the output signal at time $t$ from $\\mathbf{h}_{t}$. Both the mean and variance are of dimension (`output_size`). \n",
    "- The `forward` method also return a sample of the estimated signal.\n",
    "\n",
    "\n",
    "> **Exercise**: complete the following code. Understand all steps, particularly those in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3TP0QTf_XKq"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,sigma):\n",
    "        \n",
    "        # input size -> Dimension of the input signal\n",
    "        # outpusize -> Dimension of the output signal\n",
    "        # hidden_dim -> Dimension of the rnn state\n",
    "        # n_layers -> If >1, we are using a stacked RNN\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.sigma = torch.Tensor(np.array(sigma))\n",
    "\n",
    "        # define an RNN with specified parameters\n",
    "        # batch_first=True means that the first dimension of the input will be the batch_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, \n",
    "                          nonlinearity='relu',batch_first=True)\n",
    "        \n",
    "        # One linear layer to estimate mean\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size) # YOUR CODE HERE\n",
    "        # One linear layer to estimate log-variance\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_size) # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        \n",
    "        '''\n",
    "        About the shape of the different tensors ...:\n",
    "        \n",
    "        - Input signal x has shape (batch_size, seq_length, input_size)\n",
    "        - The initialization of the RNN hidden state h0 has shape (n_layers, batch_size, hidden_dim).\n",
    "          If None value is used, internally it is initialized to zeros.\n",
    "        - The RNN output (batch_size, seq_length, hidden_size). This output is the RNN state along time  \n",
    "\n",
    "        '''\n",
    "        \n",
    "        batch_size = x.size(0) # Number of signals N\n",
    "        seq_length = x.size(1) # T\n",
    "        \n",
    "        # get RNN outputs\n",
    "        # r_out is the sequence of states\n",
    "        # hidden is just the last state (we will use it for forecasting)\n",
    "        r_out, hidden = self.rnn(x, h0)\n",
    "        \n",
    "        # shape r_out to be (seq_length, hidden_dim) #UNDERSTANDING THIS POINT IS IMPORTANT!!        \n",
    "        r_out = r_out.reshape(-1, self.hidden_dim) \n",
    "        \n",
    "        # We compute the mean\n",
    "        mean = self.fc1(#YOUR CODE HERE)\n",
    "             \n",
    "        # We compute the variance\n",
    "        variance = torch.exp(self.fc2(#YOUR CODE HERE))+torch.ones(mean.shape)*self.sigma\n",
    "        \n",
    "        # We generate noise of the adecuate variance\n",
    "        noise = torch.randn_like(mean)*torch.sqrt(variance)\n",
    "        \n",
    "        sample = mean+noise\n",
    "        \n",
    "        # reshape back to temporal structure\n",
    "        sample = sample.reshape([-1,seq_length,int(self.output_size)])\n",
    "        mean = mean.reshape([-1,seq_length,int(self.output_size)])\n",
    "        variance = variance.reshape([-1,seq_length,int(self.output_size)])\n",
    "        \n",
    "        return mean, variance, hidden, sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAEjYHhU_XKr"
   },
   "source": [
    "> **Exercise:** Instantiate the object RNN with the right parameters for our problem. Use `hidden_dim=32`, `n_layers=1` and `sigma=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "motnQngH_XKr"
   },
   "outputs": [],
   "source": [
    "# test that dimensions are as expected\n",
    "my_rnn = #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ais5m0gA_XKr"
   },
   "source": [
    "In the following code, we compute the model output using the `forward` method. Note that we use an all zero initial state.\n",
    "\n",
    "> **Exercise**: Using the `forward` method, compare the signal predicted mean against the true value during the first 20 time instants for one of the signals. Note that the parameters of the RNN have not been trained...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6IrPNSy_XKs"
   },
   "outputs": [],
   "source": [
    "X_in = torch.Tensor(X).view([-1,T,1])\n",
    "\n",
    "mean,VAR,h,sample = my_rnn.forward(###) # YOUR CODE HERE\n",
    "\n",
    "print(f'La dimensión del estado es')\n",
    "print(h.shape)\n",
    "\n",
    "print(f'\\nLa dimensión de predicción es')\n",
    "print(mean.shape)\n",
    "\n",
    "output_no_train = mean.detach().numpy().reshape([N,-1])\n",
    "\n",
    "# Instante incial para la representación\n",
    "t0 = 0\n",
    "\n",
    "# Instante final\n",
    "tf = 50\n",
    "\n",
    "signal = 1 # Ejemplo de señal\n",
    "\n",
    "# Plot the first training signal and the target\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.arange(t0,tf,1), X[signal,t0:tf], 'r.--', label='input, x',ms=10) \n",
    "plt.plot(np.arange(t0,tf,1), Y[signal,t0:tf], 'b.-', label='target, y',ms=10) \n",
    "plt.plot(np.arange(t0,tf,1), output_no_train[signal,t0:tf], 'k.-', label='RNN output (mean)',ms=10) \n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-eOT7Z0_XKs"
   },
   "source": [
    "> **Exercise:** Complete the code for the following class, which extends `RNN` to include a training method. \n",
    "\n",
    "Note that there is no mini-batch, we process all signals for every SGD iteration. You are free to the mini-batch training functionality. \n",
    "Another novelty, typically associated with RNNs, is limiting the absolute value of the gradients (*gradient clipping*) to a preset value (2.0 in the code). When processing very long sequences, the product of terms associated with *backpropagation* can trigger gradients that are too large ([*exploiding gradients*](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf)) that cause instabilities in gradient descent. With *gradient clipping* we prevent this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1nRV--g_XKs"
   },
   "outputs": [],
   "source": [
    "class RNN_extended(RNN):\n",
    "    \n",
    "    #Your code here\n",
    "    \n",
    "    def __init__(self, num_data_train, num_iter, sequence_length,\n",
    "                 input_size, output_size, hidden_dim, n_layers, sigma, lr=0.001):\n",
    "        \n",
    "        super().__init__(input_size, output_size, hidden_dim, n_layers,sigma) \n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.num_layers = n_layers\n",
    "        \n",
    "        self.lr = lr #Learning Rate\n",
    "        \n",
    "        self.num_train = num_data_train #Number of training signals\n",
    "        \n",
    "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "        \n",
    "        self.num_iter = num_iter\n",
    "        \n",
    "        self.criterion =       #YOUR CODE HERE     \n",
    "        \n",
    "        # A list to store the loss evolution along training\n",
    "        \n",
    "        self.loss_during_training = [] \n",
    "        \n",
    "           \n",
    "    def trainloop(self,x,y):\n",
    "        \n",
    "        # SGD Loop\n",
    "        \n",
    "        for e in range(int(self.num_iter)):\n",
    "        \n",
    "            self.optim.zero_grad() \n",
    "                \n",
    "            x = torch.Tensor(x).view([self.num_train,self.sequence_length,1]) \n",
    "\n",
    "            y = torch.Tensor(y).view([self.num_train,self.sequence_length,1]) \n",
    "\n",
    "            mean,var,hid,sample = ####  #YOUR CODE HERE \n",
    "                \n",
    "            loss = self.criterion(###,sample) #YOUR CODE HERE \n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # This code helps to avoid vanishing exploiting gradients in RNNs\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
    "                \n",
    "            self.optim.step()\n",
    "                \n",
    "            self.loss_during_training.append(loss.item())\n",
    "\n",
    "            if(e % 50 == 0): # Every 10 iterations\n",
    "\n",
    "                print(\"Iteration %d. Training loss: %f\" %(e,self.loss_during_training[-1]))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eTibq0K_XKt"
   },
   "source": [
    "> **Exercise:** Using only the first 100 values of every signal, train the RNN for 200 SGD iterations. Use `hidden_dim=32`, `n_layers=1` and `sigma=0.1`. Recall that the target signal is stored in the variable `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsc-bh5N_XKt"
   },
   "outputs": [],
   "source": [
    "T_train = 100\n",
    "\n",
    "my_rnn = RNN_extended(num_data_train=X.shape[0],num_iter=200,sequence_length=T_train,\n",
    "                     input_size=1,output_size=1,hidden_dim=32,n_layers=1,sigma=0.1,lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VFw1dJ2_XKt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_rnn.trainloop(###,Y[:,:T_train]) #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anHESJtk_XKu"
   },
   "source": [
    "> **Exercise:** Plot the loss function along training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "LsZZnfIz_XKu",
    "outputId": "e133ed59-50a1-49b7-f9e3-2abbc7bb149e"
   },
   "outputs": [],
   "source": [
    "plt.plot(my_rnn.loss_during_training,label='Training Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iEhUTgV_XKu"
   },
   "source": [
    "> **Exercise:** Compute the following plot, in which we plot one of the input signals, the target one, and the prediction (mean $\\pm$ two stds) by the RNN from t=20 to t=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "b2_UXFYj_XKv",
    "outputId": "88f99371-a6b8-4c2f-c9c1-a79e18de6043"
   },
   "outputs": [],
   "source": [
    "# We first evaluate the model for the N signals up to time T_train = 100\n",
    "X_in = torch.Tensor(X[:,:T_train]).view([N,T_train,1]) \n",
    "\n",
    "mean,var,h,sample = my_rnn.forward(###) # YOUR CODE HERE\n",
    "\n",
    "mean_rnn = mean.detach().numpy().reshape([N,-1])\n",
    "std_rnn = np.sqrt(var.detach().numpy().reshape([N,-1]))\n",
    "\n",
    "# Initial time point of the representation\n",
    "t0 = 20\n",
    "\n",
    "# Final point\n",
    "tf = 70\n",
    "\n",
    "signal = 10 # From 1 to N (you can play with this)\n",
    "\n",
    "# Plot the first training signal and the target\n",
    "plt.figure(figsize=(8,5))\n",
    "#plt.plot(np.arange(t0,tf,1), X[signal,t0:tf], 'r.--', label='input, x',ms=10) # x\n",
    "plt.plot(np.arange(t0,tf,1), Y[signal,t0:tf], 'b.-', label='target, y',ms=10) # x\n",
    "plt.errorbar(np.arange(t0,tf,1), mean_rnn[signal,t0:tf], 2*std_rnn[signal,t0:tf], linestyle='None', marker='^',ecolor='tab:olive',label='prediction')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAZiyqv7_XKv"
   },
   "source": [
    "Observe that the prediction is pretty good!The RNN model have clearly learnt the dynamics of the dataset. In the previous experiment, note we have fed the RNN model with the **true** values of the signal, i.e. we have used the full signal $X$ to compute the sequence of states. \n",
    "\n",
    "Using the model we have just trained, lets do now **forecasting**. Namely, we feed the RNN the output that we predicted and we do this recursively for as long as we want. This represents **sampling** from the probabilistic model \n",
    "\n",
    "$$p(X|X_0) = \\prod_{t=1}^T p(X_t|\\mathbf{X}_{0:t-1})$$\n",
    "$$p(X_t|X_{t-1},\\mathbf{X}_{0:t-2}) = \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2+g_{RNN}(X_{t-1},\\mathbf{h}_{t-1})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhjyiQ1G_XKv"
   },
   "source": [
    "To do forecasting, note that we have to recursively call the `forward` method and feed the obtained RNN output and state as the entry and initial state for the next `forward` call. The following code would do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heeUqWjs_XKw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We take the last output of the RNN for the N signals at time T_train\n",
    "current_input = sample[:,-1,:].view([N,1,1]) \n",
    "# And the last state\n",
    "current_state = h\n",
    "\n",
    "# Here we store the sampled signals\n",
    "forecast_rnn = np.zeros([N,T-T_train])\n",
    "\n",
    "for t in range(T-T_train):\n",
    "    \n",
    "    # En este bucle muestreamos iterativamente el modelo\n",
    "    \n",
    "    mean,var,current_state,current_input = my_rnn.forward(current_input,#YOUR CODE HERE) \n",
    "    \n",
    "    forecast_rnn[:,t] = current_input.detach().numpy().reshape([-1,])\n",
    "\n",
    "    \n",
    "# We concatenate the original ones (until T_train) with the ones we just sampled (from T_train)    \n",
    "final_rnn_reconstruct = np.hstack([mean_rnn,forecast_rnn])\n",
    "\n",
    "# Now we plot\n",
    "\n",
    "t0 = 80\n",
    "\n",
    "tfinal = 200\n",
    "\n",
    "plt.plot(np.arange(t0,tfinal,1), Y[signal,t0:tfinal].reshape([-1]), 'b.-', label='target, y',ms=10)  \n",
    "plt.plot(np.arange(t0,tfinal,1), final_rnn_reconstruct[signal,t0:tfinal], 'r-', label='RNN output',ms=10) \n",
    "plt.plot([T_train,T_train],[np.min(Y[signal,:]),np.max(Y[signal,:])],'k--',label='forecasting begins')\n",
    "plt.legend()\n",
    "\n",
    "plt.legend(loc='best')\n",
    "print('Between t=0 and t=100, we feed the real values')\n",
    "print('From t=100, we feed the estimated values (forecasting)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_--5KXc_XKw"
   },
   "source": [
    "Observe that, after a while, the model deviates from the true realization. That is **expected** we are sampling from the generative model and it is likely that do not get exactly the same sample! Particularly in RNN, since they have short memory.\n",
    "\n",
    "## LSTMs\n",
    "\n",
    "Lets study how an LSTM would perform in this context. You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "\n",
    "where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. If **stacked LSTMs (n_layers>1) are used** we can automatically add dropout between LSTM layers with te parameter `dropout` with a specified probability.\n",
    "\n",
    "> **Exercise:** Complete the code for the following two classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_s0iG61_XKx"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,sigma,drop_prob):\n",
    "        \n",
    "        # input size -> Dimension of the input signal\n",
    "        # outpusize -> Dimension of the output signal\n",
    "        # hidden_dim -> Dimension of the rnn state\n",
    "        # n_layers -> If >1, we are using a stacked RNN\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE (Many lines!)\n",
    "\n",
    "    def forward(self, x, h0=None, valid=False):\n",
    "        \n",
    "        '''\n",
    "        About the shape of the different tensors ...:\n",
    "        \n",
    "        - Input signal x has shape (batch_size, seq_length, input_size)\n",
    "        - The initialization of the LSTM hidden state is a tuple, containing two tensors of dimensions\n",
    "          (n_layers, batch_size, hidden_dim) each. The first tensor represents the LSTM hidden state \n",
    "          cell states. We can use the None value so internally they are initialized with 0s.\n",
    "        - The LSTM output shape is (batch_size, seq_length, hidden_size) \n",
    "\n",
    "        '''\n",
    "        \n",
    "        if(valid): # To activate/deactiave dropout in validation (only needed if n_layers>1)\n",
    "            self.eval()\n",
    "        else:\n",
    "            self.train()\n",
    "        \n",
    "        batch_size = x.size(0) # Number of signals N\n",
    "        seq_length = x.size(1) # T\n",
    "        \n",
    "         # YOUR CODE HERE (Many lines!)\n",
    "        \n",
    "        return mean, variance, hidden, sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzVl0I1x_XKx"
   },
   "outputs": [],
   "source": [
    "class LSTM_extended(LSTM):\n",
    "    \n",
    "    # YOUR CODE HERE (Many lines!)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogCyz0w-_XKx"
   },
   "source": [
    "> **Exercise:** Train the LSTM model for 200 iterations using the first 100 values of each signal. Use `hidden_dim=32`, `n_layers=1` and `sigma=0.1`. Recall that the target signal is stored in the variable `Y`.\n",
    "Note that with only one layer, the dropout probability parameter does not play any role (you will get a warning actually).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlds3WUm_XKx",
    "outputId": "f603f0a6-5fc8-4e4d-9d93-0e3465a264b8"
   },
   "outputs": [],
   "source": [
    " # YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQvMt9QeC959"
   },
   "source": [
    "> **Exercise:** Plot the loss function along training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "935qXACq_XKy"
   },
   "outputs": [],
   "source": [
    " # YOUR CODE HERE (Many lines!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBt6cnKx6rgz"
   },
   "source": [
    "We can see how the cost function (MSELoss) is similar in both cases. In terms of predicting the next instant with *teaching forcing*, both models achieve similar performance. Keep in mind that the memory of the autoregressive model that we have used to generate the signals is small...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Myc5ow0F_XKz"
   },
   "source": [
    "> **Exercise:** Complete the code to visualize the LSTM prediction of the next value of the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5xrm49v_XKz"
   },
   "outputs": [],
   "source": [
    " # YOUR CODE HERE (Many lines!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK31W_Nc_XK0"
   },
   "source": [
    "> **Exercise:** Complete the code to visualize the LSTM forecasting. Plot the LSTM vs RNN vs target for a few signals and discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5bQlF3g_XK0"
   },
   "outputs": [],
   "source": [
    " # YOUR CODE HERE (Many lines!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44Y_3mqw_XK0"
   },
   "source": [
    "The conclusions are similar, since both models have captured well the dynamics of the data, generated with an autoregressive model with reduced memory. It seems LSTMs does better though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dyS9W1t_qaW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
